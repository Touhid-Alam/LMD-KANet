{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKAd3ZjgEn4Y",
        "outputId": "25f3b5b1-49c6-41bc-e737-d980fa30ad63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========================================================================================================\n",
            "LAYER / MODULE            | INPUT SHAPE          | OUTPUT SHAPE         | KERNEL   | ACTIVATION  \n",
            "=========================================================================================================\n",
            "STEM (Conv+BN)            | (1, 3, 224, 224)     | (1, 24, 112, 112)    | 3x3      | HardSwish   \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "HybridMBConv 1            | (1, 24, 112, 112)    | (1, 24, 112, 112)    | 3x3      | HardSwish   \n",
            "HybridMBConv 2            | (1, 24, 112, 112)    | (1, 40, 56, 56)      | 3x3      | HardSwish   \n",
            "HybridMBConv 3            | (1, 40, 56, 56)      | (1, 40, 56, 56)      | 3x3      | HardSwish   \n",
            "HybridMBConv 4            | (1, 40, 56, 56)      | (1, 80, 28, 28)      | 5x5      | HardSwish   \n",
            "HybridMBConv 5            | (1, 80, 28, 28)      | (1, 80, 28, 28)      | 5x5      | HardSwish   \n",
            "HybridMBConv 6            | (1, 80, 28, 28)      | (1, 80, 28, 28)      | 5x5      | HardSwish   \n",
            "HybridMBConv 7            | (1, 80, 28, 28)      | (1, 112, 28, 28)     | 3x3      | HardSwish   \n",
            "HybridMBConv 8            | (1, 112, 28, 28)     | (1, 160, 14, 14)     | 5x5      | HardSwish   \n",
            "HybridMBConv 9            | (1, 160, 14, 14)     | (1, 160, 14, 14)     | 5x5      | HardSwish   \n",
            "HybridMBConv 10           | (1, 160, 14, 14)     | (1, 320, 14, 14)     | 3x3      | HardSwish   \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "FINAL EXPANSION           | (1, 320, 14, 14)     | (1, 1280, 14, 14)    | 1x1      | HardSwish   \n",
            "=========================================================================================================\n",
            "COORDINATE ATTN           | (1, 1280, 14, 14)    | (1, 1280, 14, 14)    | 1x1 (Red) | Sigmoid     \n",
            "SIMAM (ENERGY)            | (1, 1280, 14, 14)    | (1, 1280, 14, 14)    | None     | Sigmoid     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "GLOBAL POOLING            | (1, 1280, 14, 14)    | (1, 1280)            | Avg      | -           \n",
            "DROPOUT (p=0.3)           | (1, 1280)            | (1, 1280)            | -        | -           \n",
            "KAN CLASSIFIER            | (1, 1280)            | (1, 4)               | Spline   | B-Spline    \n",
            "=========================================================================================================\n",
            "OPTIMIZER CONFIGURATION (For Training Phase)\n",
            " - Optimizer: AdamW\n",
            " - Learning Rate: 1e-4\n",
            " - Scheduler: ReduceLROnPlateau (Factor 0.5, Patience 2)\n",
            " - Loss Function: CrossEntropyLoss\n",
            "=========================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "# ==================================================================================\n",
        "# 1. DEFINE THE LMD-KANet MODEL (Required for the summary to work)\n",
        "# ==================================================================================\n",
        "\n",
        "# --- Activations ---\n",
        "class HardSwish(nn.Module):\n",
        "    def forward(self, x): return x * F.relu6(x + 3.) / 6.\n",
        "\n",
        "# --- Components ---\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, in_c, r_dim):\n",
        "        super().__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_c, r_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(r_dim, in_c, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x): return x * self.se(x)\n",
        "\n",
        "class HybridMBConv(nn.Module):\n",
        "    def __init__(self, in_c, out_c, k, s, exp):\n",
        "        super().__init__()\n",
        "        self.config = {'k': k, 's': s, 'exp': exp} # Store for summary\n",
        "        self.use_res = (s == 1 and in_c == out_c)\n",
        "        hid = int(in_c * exp)\n",
        "        layers = []\n",
        "        if exp != 1: layers.extend([nn.Conv2d(in_c, hid, 1, bias=False), nn.BatchNorm2d(hid), HardSwish()])\n",
        "        pad = (k - 1) // 2\n",
        "        layers.extend([nn.Conv2d(hid, hid, k, s, pad, groups=hid, bias=False), nn.BatchNorm2d(hid), HardSwish()])\n",
        "        layers.append(SqueezeExcitation(hid, hid // 4))\n",
        "        layers.extend([nn.Conv2d(hid, out_c, 1, bias=False), nn.BatchNorm2d(out_c)])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "    def forward(self, x): return x + self.conv(x) if self.use_res else self.conv(x)\n",
        "\n",
        "class EffiMobileBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(nn.Conv2d(3, 24, 3, 2, 1, bias=False), nn.BatchNorm2d(24), HardSwish())\n",
        "        # Config: [in, out, k, s, exp]\n",
        "        self.layers_config = [\n",
        "            [24, 24, 3, 1, 1], [24, 40, 3, 2, 4], [40, 40, 3, 1, 4],\n",
        "            [40, 80, 5, 2, 6], [80, 80, 5, 1, 6], [80, 80, 5, 1, 6],\n",
        "            [80, 112, 3, 1, 6], [112, 160, 5, 2, 6], [160, 160, 5, 1, 6], [160, 320, 3, 1, 6]\n",
        "        ]\n",
        "        self.blocks = nn.Sequential(*[HybridMBConv(*c) for c in self.layers_config])\n",
        "        self.final_conv = nn.Sequential(nn.Conv2d(320, 1280, 1, bias=False), nn.BatchNorm2d(1280), HardSwish())\n",
        "    def forward(self, x): return self.final_conv(self.blocks(self.stem(x)))\n",
        "\n",
        "class CoordinateAttention(nn.Module):\n",
        "    def __init__(self, inp, oup):\n",
        "        super().__init__()\n",
        "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1)); self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
        "        mip = max(8, inp // 32)\n",
        "        self.conv1 = nn.Conv2d(inp, mip, 1); self.bn1 = nn.BatchNorm2d(mip); self.act = HardSwish()\n",
        "        self.conv_h = nn.Conv2d(mip, oup, 1); self.conv_w = nn.Conv2d(mip, oup, 1)\n",
        "    def forward(self, x):\n",
        "        x_h, x_w = self.pool_h(x), self.pool_w(x).permute(0, 1, 3, 2)\n",
        "        y = self.act(self.bn1(self.conv1(torch.cat([x_h, x_w], dim=2))))\n",
        "        x_h, x_w = torch.split(y, [x.size(2), x.size(3)], dim=2)\n",
        "        return x * torch.sigmoid(self.conv_h(x_h)) * torch.sigmoid(self.conv_w(x_w.permute(0, 1, 3, 2)))\n",
        "\n",
        "class SimAM(nn.Module):\n",
        "    def __init__(self, e_lambda=1e-4):\n",
        "        super().__init__(); self.activaton = nn.Sigmoid(); self.e_lambda = e_lambda\n",
        "    def forward(self, x):\n",
        "        n = x.shape[2] * x.shape[3] - 1\n",
        "        d = (x - x.mean(dim=[2,3], keepdim=True)).pow(2)\n",
        "        v = d / (4 * (d.sum(dim=[2,3], keepdim=True) / n + self.e_lambda)) + 0.5\n",
        "        return x * self.activaton(v)\n",
        "\n",
        "class KANLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=5):\n",
        "        super().__init__()\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size))\n",
        "        nn.init.kaiming_uniform_(self.base_weight, a=5**0.5); nn.init.normal_(self.spline_weight, 0.0, 1.0)\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.base_weight) + torch.sum(self.spline_weight.mean(dim=2) * x.unsqueeze(1), dim=2)\n",
        "\n",
        "class LMD_KANet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = EffiMobileBackbone()\n",
        "        self.coord = CoordinateAttention(1280, 1280)\n",
        "        self.simam = SimAM()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1); self.drop = nn.Dropout(0.3)\n",
        "        self.kan = KANLinear(1280, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.simam(self.coord(x))\n",
        "        return self.kan(self.drop(self.pool(x).flatten(1)))\n",
        "\n",
        "# ==================================================================================\n",
        "# 2. CUSTOM SUMMARY GENERATOR (The \"Code Man Summary\")\n",
        "# ==================================================================================\n",
        "def generate_deep_summary(model, input_size=(1, 3, 224, 224)):\n",
        "    print(\"\\n\" + \"=\"*105)\n",
        "    print(f\"{'LAYER / MODULE':<25} | {'INPUT SHAPE':<20} | {'OUTPUT SHAPE':<20} | {'KERNEL':<8} | {'ACTIVATION':<12}\")\n",
        "    print(\"=\"*105)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    x = torch.randn(input_size).to(device)\n",
        "\n",
        "    # 1. STEM\n",
        "    in_s = tuple(x.shape)\n",
        "    x = model.backbone.stem(x)\n",
        "    print(f\"{'STEM (Conv+BN)':<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {'3x3':<8} | {'HardSwish':<12}\")\n",
        "    print(\"-\" * 105)\n",
        "\n",
        "    # 2. BACKBONE BLOCKS\n",
        "    for i, block in enumerate(model.backbone.blocks):\n",
        "        in_s = tuple(x.shape)\n",
        "        x = block(x)\n",
        "\n",
        "        # Extract details\n",
        "        k = f\"{block.config['k']}x{block.config['k']}\"\n",
        "        s = block.config['s']\n",
        "        act = \"HardSwish\"\n",
        "        module_name = f\"HybridMBConv {i+1}\"\n",
        "\n",
        "        # Formatting for readability\n",
        "        print(f\"{module_name:<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {k:<8} | {act:<12}\")\n",
        "\n",
        "    print(\"-\" * 105)\n",
        "\n",
        "    # 3. FINAL CONV\n",
        "    in_s = tuple(x.shape)\n",
        "    x = model.backbone.final_conv(x)\n",
        "    print(f\"{'FINAL EXPANSION':<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {'1x1':<8} | {'HardSwish':<12}\")\n",
        "    print(\"=\" * 105)\n",
        "\n",
        "    # 4. ATTENTION\n",
        "    # Coordinate\n",
        "    in_s = tuple(x.shape)\n",
        "    x = model.coord(x)\n",
        "    print(f\"{'COORDINATE ATTN':<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {'1x1 (Red)':<8} | {'Sigmoid':<12}\")\n",
        "\n",
        "    # SimAM\n",
        "    in_s = tuple(x.shape)\n",
        "    x = model.simam(x)\n",
        "    print(f\"{'SIMAM (ENERGY)':<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {'None':<8} | {'Sigmoid':<12}\")\n",
        "\n",
        "    print(\"-\" * 105)\n",
        "\n",
        "    # 5. HEAD\n",
        "    # Pooling\n",
        "    in_s = tuple(x.shape)\n",
        "    x = model.pool(x).flatten(1)\n",
        "    print(f\"{'GLOBAL POOLING':<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {'Avg':<8} | {'-':<12}\")\n",
        "\n",
        "    # Dropout\n",
        "    in_s = tuple(x.shape)\n",
        "    x = model.drop(x)\n",
        "    print(f\"{'DROPOUT (p=0.3)':<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {'-':<8} | {'-':<12}\")\n",
        "\n",
        "    # KAN\n",
        "    in_s = tuple(x.shape)\n",
        "    x = model.kan(x)\n",
        "    print(f\"{'KAN CLASSIFIER':<25} | {str(in_s):<20} | {str(tuple(x.shape)):<20} | {'Spline':<8} | {'B-Spline':<12}\")\n",
        "\n",
        "    print(\"=\" * 105)\n",
        "    print(f\"OPTIMIZER CONFIGURATION (For Training Phase)\")\n",
        "    print(f\" - Optimizer: AdamW\")\n",
        "    print(f\" - Learning Rate: 1e-4\")\n",
        "    print(f\" - Scheduler: ReduceLROnPlateau (Factor 0.5, Patience 2)\")\n",
        "    print(f\" - Loss Function: CrossEntropyLoss\")\n",
        "    print(\"=\" * 105)\n",
        "\n",
        "# ==================================================================================\n",
        "# 3. RUN IT\n",
        "# ==================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Initialize with 4 classes (Chest CT example)\n",
        "    model = LMD_KANet(num_classes=4).to(device)\n",
        "\n",
        "    generate_deep_summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==================================================================================\n",
        "# 1. MODEL DEFINITION (Must be included to inspect it)\n",
        "# ==================================================================================\n",
        "class HardSwish(nn.Module):\n",
        "    def forward(self, x): return x * F.relu6(x + 3.) / 6.\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, in_c, r_dim):\n",
        "        super().__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_c, r_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(r_dim, in_c, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x): return x * self.se(x)\n",
        "\n",
        "class HybridMBConv(nn.Module):\n",
        "    def __init__(self, in_c, out_c, k, s, exp):\n",
        "        super().__init__()\n",
        "        self.use_res = (s == 1 and in_c == out_c)\n",
        "        hid = int(in_c * exp)\n",
        "\n",
        "        # We define layers individually to make them accessible for inspection\n",
        "        self.expand_conv = None\n",
        "        if exp != 1:\n",
        "            self.expand_conv = nn.Sequential(nn.Conv2d(in_c, hid, 1, bias=False), nn.BatchNorm2d(hid), HardSwish())\n",
        "\n",
        "        self.depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(hid, hid, k, s, (k-1)//2, groups=hid, bias=False),\n",
        "            nn.BatchNorm2d(hid), HardSwish()\n",
        "        )\n",
        "        self.se_block = SqueezeExcitation(hid, hid // 4)\n",
        "        self.proj_conv = nn.Sequential(nn.Conv2d(hid, out_c, 1, bias=False), nn.BatchNorm2d(out_c))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.expand_conv(x) if self.expand_conv else x\n",
        "        out = self.depth_conv(out)\n",
        "        out = self.se_block(out)\n",
        "        out = self.proj_conv(out)\n",
        "        return x + out if self.use_res else out\n",
        "\n",
        "class EffiMobileBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(nn.Conv2d(3, 24, 3, 2, 1, bias=False), nn.BatchNorm2d(24), HardSwish())\n",
        "        config = [\n",
        "            [24, 24, 3, 1, 1], [24, 40, 3, 2, 4], [40, 40, 3, 1, 4],\n",
        "            [40, 80, 5, 2, 6], [80, 80, 5, 1, 6], [80, 80, 5, 1, 6],\n",
        "            [80, 112, 3, 1, 6], [112, 160, 5, 2, 6], [160, 160, 5, 1, 6], [160, 320, 3, 1, 6]\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*[HybridMBConv(*c) for c in config])\n",
        "        self.final_conv = nn.Sequential(nn.Conv2d(320, 1280, 1, bias=False), nn.BatchNorm2d(1280), HardSwish())\n",
        "    def forward(self, x): return self.final_conv(self.layers(self.stem(x)))\n",
        "\n",
        "class CoordinateAttention(nn.Module):\n",
        "    def __init__(self, inp, oup):\n",
        "        super().__init__()\n",
        "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
        "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
        "        mip = max(8, inp // 32)\n",
        "        self.conv1 = nn.Conv2d(inp, mip, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(mip)\n",
        "        self.act = HardSwish()\n",
        "        self.conv_h = nn.Conv2d(mip, oup, 1)\n",
        "        self.conv_w = nn.Conv2d(mip, oup, 1)\n",
        "    def forward(self, x):\n",
        "        # Dummy forward for shape tracing\n",
        "        return x\n",
        "\n",
        "class SimAM(nn.Module):\n",
        "    def __init__(self, e_lambda=1e-4):\n",
        "        super().__init__()\n",
        "        self.activaton = nn.Sigmoid()\n",
        "    def forward(self, x): return x\n",
        "\n",
        "class KANLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=5):\n",
        "        super().__init__()\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size))\n",
        "    def forward(self, x): return x\n",
        "\n",
        "class LMD_KANet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = EffiMobileBackbone()\n",
        "        self.coord = CoordinateAttention(1280, 1280)\n",
        "        self.simam = SimAM()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1); self.drop = nn.Dropout(0.3)\n",
        "        self.kan = KANLinear(1280, num_classes)\n",
        "    def forward(self, x): return x\n",
        "\n",
        "# ==================================================================================\n",
        "# 2. DEEP INSPECTOR FUNCTION\n",
        "# ==================================================================================\n",
        "def print_recursive_summary(model):\n",
        "    print(f\"{'MODULE / LAYER':<40} | {'KERNEL':<8} | {'STRIDE':<8} | {'ACTIVATION':<15} | {'PARAMS'}\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    def count_p(m): return sum(p.numel() for p in m.parameters())\n",
        "\n",
        "    # 1. STEM\n",
        "    print(\">> [1] STEM\")\n",
        "    s = model.backbone.stem\n",
        "    print(f\"{'  Conv2d (Input->24)':<40} | {'3x3':<8} | {'2':<8} | {'-':<15} | {count_p(s[0]):,}\")\n",
        "    print(f\"{'  BatchNorm2d':<40} | {'-':<8} | {'-':<8} | {'-':<15} | {count_p(s[1]):,}\")\n",
        "    print(f\"{'  HardSwish':<40} | {'-':<8} | {'-':<8} | {'HardSwish':<15} | 0\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    # 2. BACKBONE BLOCKS (Sample Block 4 as it is complex)\n",
        "    print(\">> [2] HYBRID MB-CONV BLOCKS (Example: Block 4 - 5x5 Kernel)\")\n",
        "    # We inspect the 4th block (index 3) manually to show structure\n",
        "    b = model.backbone.layers[3]\n",
        "\n",
        "    # Expand\n",
        "    print(f\"{'  [Expansion Phase]':<40}\")\n",
        "    print(f\"{'    Conv2d (1x1)':<40} | {'1x1':<8} | {'1':<8} | {'-':<15} | {count_p(b.expand_conv[0]):,}\")\n",
        "    print(f\"{'    BatchNorm2d':<40} | {'-':<8} | {'-':<8} | {'HardSwish':<15} | {count_p(b.expand_conv[1]):,}\")\n",
        "\n",
        "    # Depthwise\n",
        "    print(f\"{'  [Depthwise Phase]':<40}\")\n",
        "    print(f\"{'    Conv2d (5x5, Groups=C)':<40} | {'5x5':<8} | {'2':<8} | {'-':<15} | {count_p(b.depth_conv[0]):,}\")\n",
        "    print(f\"{'    BatchNorm2d':<40} | {'-':<8} | {'-':<8} | {'HardSwish':<15} | {count_p(b.depth_conv[1]):,}\")\n",
        "\n",
        "    # SE\n",
        "    print(f\"{'  [Squeeze-Excitation]':<40}\")\n",
        "    print(f\"{'    AdaptiveAvgPool2d':<40} | {'-':<8} | {'-':<8} | {'-':<15} | 0\")\n",
        "    print(f\"{'    Conv2d (Squeeze)':<40} | {'1x1':<8} | {'1':<8} | {'SiLU':<15} | {count_p(b.se_block.se[1]):,}\")\n",
        "    print(f\"{'    Conv2d (Excite)':<40} | {'1x1':<8} | {'1':<8} | {'Sigmoid':<15} | {count_p(b.se_block.se[3]):,}\")\n",
        "\n",
        "    # Project\n",
        "    print(f\"{'  [Projection Phase]':<40}\")\n",
        "    print(f\"{'    Conv2d (1x1)':<40} | {'1x1':<8} | {'1':<8} | {'-':<15} | {count_p(b.proj_conv[0]):,}\")\n",
        "    print(f\"{'    BatchNorm2d':<40} | {'-':<8} | {'-':<8} | {'Linear':<15} | {count_p(b.proj_conv[1]):,}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    # 3. ATTENTION\n",
        "    print(\">> [3] COORDINATE ATTENTION\")\n",
        "    c = model.coord\n",
        "    print(f\"{'  Pool_H + Pool_W':<40} | {'Avg':<8} | {'-':<8} | {'-':<15} | 0\")\n",
        "    print(f\"{'  Conv2d (Shared)':<40} | {'1x1':<8} | {'1':<8} | {'HardSwish':<15} | {count_p(c.conv1):,}\")\n",
        "    print(f\"{'  Conv2d (X-Attn)':<40} | {'1x1':<8} | {'1':<8} | {'Sigmoid':<15} | {count_p(c.conv_h):,}\")\n",
        "    print(f\"{'  Conv2d (Y-Attn)':<40} | {'1x1':<8} | {'1':<8} | {'Sigmoid':<15} | {count_p(c.conv_w):,}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    print(\">> [4] SIMAM ATTENTION\")\n",
        "    print(f\"{'  Energy Calculation':<40} | {'-':<8} | {'-':<8} | {'-':<15} | 0\")\n",
        "    print(f\"{'  Sigmoid Activation':<40} | {'-':<8} | {'-':<8} | {'Sigmoid':<15} | 0\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    # 4. KAN HEAD\n",
        "    print(\">> [5] KAN CLASSIFICATION HEAD\")\n",
        "    k = model.kan\n",
        "    print(f\"{'  Global Avg Pool':<40} | {'-':<8} | {'-':<8} | {'-':<15} | 0\")\n",
        "    print(f\"{'  Dropout (p=0.3)':<40} | {'-':<8} | {'-':<8} | {'-':<15} | 0\")\n",
        "    print(f\"{'  KAN Linear (Base Weights)':<40} | {'Linear':<8} | {'-':<8} | {'SiLU':<15} | {k.base_weight.numel():,}\")\n",
        "    print(f\"{'  KAN Linear (Spline Grid)':<40} | {'Spline':<8} | {'-':<8} | {'B-Spline':<15} | {k.spline_weight.numel():,}\")\n",
        "\n",
        "    print(\"=\" * 90)\n",
        "    print(f\"TOTAL MODEL PARAMS: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = LMD_KANet(num_classes=4)\n",
        "    print_recursive_summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQlMpfx6QvWc",
        "outputId": "82d531e9-cc02-4678-9a77-8019b7e3311b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODULE / LAYER                           | KERNEL   | STRIDE   | ACTIVATION      | PARAMS\n",
            "==========================================================================================\n",
            ">> [1] STEM\n",
            "  Conv2d (Input->24)                     | 3x3      | 2        | -               | 648\n",
            "  BatchNorm2d                            | -        | -        | -               | 48\n",
            "  HardSwish                              | -        | -        | HardSwish       | 0\n",
            "------------------------------------------------------------------------------------------\n",
            ">> [2] HYBRID MB-CONV BLOCKS (Example: Block 4 - 5x5 Kernel)\n",
            "  [Expansion Phase]                     \n",
            "    Conv2d (1x1)                         | 1x1      | 1        | -               | 9,600\n",
            "    BatchNorm2d                          | -        | -        | HardSwish       | 480\n",
            "  [Depthwise Phase]                     \n",
            "    Conv2d (5x5, Groups=C)               | 5x5      | 2        | -               | 6,000\n",
            "    BatchNorm2d                          | -        | -        | HardSwish       | 480\n",
            "  [Squeeze-Excitation]                  \n",
            "    AdaptiveAvgPool2d                    | -        | -        | -               | 0\n",
            "    Conv2d (Squeeze)                     | 1x1      | 1        | SiLU            | 14,460\n",
            "    Conv2d (Excite)                      | 1x1      | 1        | Sigmoid         | 14,640\n",
            "  [Projection Phase]                    \n",
            "    Conv2d (1x1)                         | 1x1      | 1        | -               | 19,200\n",
            "    BatchNorm2d                          | -        | -        | Linear          | 160\n",
            "------------------------------------------------------------------------------------------\n",
            ">> [3] COORDINATE ATTENTION\n",
            "  Pool_H + Pool_W                        | Avg      | -        | -               | 0\n",
            "  Conv2d (Shared)                        | 1x1      | 1        | HardSwish       | 51,240\n",
            "  Conv2d (X-Attn)                        | 1x1      | 1        | Sigmoid         | 52,480\n",
            "  Conv2d (Y-Attn)                        | 1x1      | 1        | Sigmoid         | 52,480\n",
            "------------------------------------------------------------------------------------------\n",
            ">> [4] SIMAM ATTENTION\n",
            "  Energy Calculation                     | -        | -        | -               | 0\n",
            "  Sigmoid Activation                     | -        | -        | Sigmoid         | 0\n",
            "------------------------------------------------------------------------------------------\n",
            ">> [5] KAN CLASSIFICATION HEAD\n",
            "  Global Avg Pool                        | -        | -        | -               | 0\n",
            "  Dropout (p=0.3)                        | -        | -        | -               | 0\n",
            "  KAN Linear (Base Weights)              | Linear   | -        | SiLU            | 5,120\n",
            "  KAN Linear (Spline Grid)               | Spline   | -        | B-Spline        | 25,600\n",
            "==========================================================================================\n",
            "TOTAL MODEL PARAMS: 3,496,530\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Helper Classes (Same as your model) ---\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, in_c, r_dim):\n",
        "        super().__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_c, r_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(r_dim, in_c, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x): return x * self.se(x)\n",
        "\n",
        "def inspect_se_and_convs():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"      DETAILED INSPECTION: SE & CONVOLUTION INTERNALS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. SETUP DUMMY DATA\n",
        "    # Simulating a block deep in the network (e.g., Block 8)\n",
        "    # Input: 112 channels, Expanded to 672 (6x), Kernel 5x5\n",
        "    in_c = 112\n",
        "    expand_ratio = 6\n",
        "    hidden_dim = in_c * expand_ratio # 672\n",
        "    kernel = 5\n",
        "\n",
        "    x = torch.randn(1, hidden_dim, 14, 14) # Dummy input for depthwise/SE phase\n",
        "\n",
        "    print(f\"Context: Analyzing Block 8 (Deep Layer)\")\n",
        "    print(f\"Input Channels: {in_c} -> Expanded: {hidden_dim}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # 2. INSPECT DEPTHWISE CONV\n",
        "    depth_conv = nn.Conv2d(hidden_dim, hidden_dim, kernel, 1, 2, groups=hidden_dim, bias=False)\n",
        "    params_depth = sum(p.numel() for p in depth_conv.parameters())\n",
        "\n",
        "    print(f\"[A] Depthwise Convolution ({kernel}x{kernel})\")\n",
        "    print(f\"    - Input Shape:  {list(x.shape)}\")\n",
        "    print(f\"    - Groups:       {hidden_dim} (1 filter per channel)\")\n",
        "    print(f\"    - Params:       {kernel} * {kernel} * {hidden_dim} = {params_depth:,}\")\n",
        "    print(f\"    - Logic:        Spatial Filtering only (Shape extraction)\")\n",
        "\n",
        "    # 3. INSPECT SQUEEZE-AND-EXCITATION\n",
        "    se = SqueezeExcitation(hidden_dim, hidden_dim // 4)\n",
        "    x_se = se(x)\n",
        "\n",
        "    # Analyze internal weights of SE\n",
        "    sq_conv = se.se[1] # Reduction layer\n",
        "    ex_conv = se.se[3] # Expansion layer\n",
        "    p_sq = sum(p.numel() for p in sq_conv.parameters())\n",
        "    p_ex = sum(p.numel() for p in ex_conv.parameters())\n",
        "\n",
        "    print(f\"\\n[B] Squeeze-and-Excitation (SE)\")\n",
        "    print(f\"    - Input:        {list(x.shape)}\")\n",
        "    print(f\"    - 1. AvgPool:   [1, {hidden_dim}, 1, 1] (Global Descriptor)\")\n",
        "    print(f\"    - 2. Squeeze:   1x1 Conv ({hidden_dim}->{hidden_dim//4}) | Act: SiLU | Params: {p_sq}\")\n",
        "    print(f\"    - 3. Excite:    1x1 Conv ({hidden_dim//4}->{hidden_dim}) | Act: Sigmoid | Params: {p_ex}\")\n",
        "    print(f\"    - 4. Scale:     Element-wise Multiplication\")\n",
        "    print(f\"    - Output:       {list(x_se.shape)}\")\n",
        "\n",
        "    # 4. INSPECT POINTWISE PROJECTION\n",
        "    out_c = 160 # Target output for Block 8\n",
        "    proj_conv = nn.Conv2d(hidden_dim, out_c, 1, bias=False)\n",
        "    p_proj = sum(p.numel() for p in proj_conv.parameters())\n",
        "\n",
        "    print(f\"\\n[C] Pointwise Projection (1x1)\")\n",
        "    print(f\"    - Input:        {list(x_se.shape)}\")\n",
        "    print(f\"    - Kernel:       1x1\")\n",
        "    print(f\"    - Operation:    Linear Combination of channels\")\n",
        "    print(f\"    - Params:       {hidden_dim} * {out_c} = {p_proj:,}\")\n",
        "    print(f\"    - Activation:   None (Linear bottleneck to preserve info)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inspect_se_and_convs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMo92umFT2pX",
        "outputId": "b6b51659-e769-444d-d381-cf32c841f158"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "      DETAILED INSPECTION: SE & CONVOLUTION INTERNALS\n",
            "================================================================================\n",
            "Context: Analyzing Block 8 (Deep Layer)\n",
            "Input Channels: 112 -> Expanded: 672\n",
            "--------------------------------------------------------------------------------\n",
            "[A] Depthwise Convolution (5x5)\n",
            "    - Input Shape:  [1, 672, 14, 14]\n",
            "    - Groups:       672 (1 filter per channel)\n",
            "    - Params:       5 * 5 * 672 = 16,800\n",
            "    - Logic:        Spatial Filtering only (Shape extraction)\n",
            "\n",
            "[B] Squeeze-and-Excitation (SE)\n",
            "    - Input:        [1, 672, 14, 14]\n",
            "    - 1. AvgPool:   [1, 672, 1, 1] (Global Descriptor)\n",
            "    - 2. Squeeze:   1x1 Conv (672->168) | Act: SiLU | Params: 113064\n",
            "    - 3. Excite:    1x1 Conv (168->672) | Act: Sigmoid | Params: 113568\n",
            "    - 4. Scale:     Element-wise Multiplication\n",
            "    - Output:       [1, 672, 14, 14]\n",
            "\n",
            "[C] Pointwise Projection (1x1)\n",
            "    - Input:        [1, 672, 14, 14]\n",
            "    - Kernel:       1x1\n",
            "    - Operation:    Linear Combination of channels\n",
            "    - Params:       672 * 160 = 107,520\n",
            "    - Activation:   None (Linear bottleneck to preserve info)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==================================================================================\n",
        "# 1. DEEP INSPECTION: COORDINATE ATTENTION (The \"Where\")\n",
        "# ==================================================================================\n",
        "def inspect_coordinate_attention():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"   1. COORDINATE ATTENTION: INNER MECHANICS (Step-by-Step)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Setup Dummy Input (Simulating a feature map deep in the network)\n",
        "    # Batch=1, Channels=32, Height=14, Width=14\n",
        "    x = torch.randn(1, 32, 14, 14)\n",
        "    n, c, h, w = x.size()\n",
        "    print(f\"[INPUT] Feature Map: {list(x.shape)}\")\n",
        "\n",
        "    # 2. Define the Pooling Layers\n",
        "    # pool_h: Keeps Height, Squeezes Width to 1\n",
        "    pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
        "    # pool_w: Keeps Width, Squeezes Height to 1\n",
        "    pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
        "\n",
        "    # 3. Perform Pooling\n",
        "    x_h = pool_h(x)\n",
        "    x_w = pool_w(x)\n",
        "\n",
        "    print(f\"\\n--- A. Direction-Aware Pooling ---\")\n",
        "    print(f\"   > pool_h(x): {list(x_h.shape)}  <- Compresses Width, keeps Row information\")\n",
        "    print(f\"   > pool_w(x): {list(x_w.shape)}  <- Compresses Height, keeps Column information\")\n",
        "\n",
        "    # 4. Concatenation\n",
        "    # We must permute x_w to stack it with x_h\n",
        "    x_w_perm = x_w.permute(0, 1, 3, 2) # [1, 32, 14, 1]\n",
        "\n",
        "    x_cat = torch.cat([x_h, x_w_perm], dim=2)\n",
        "    print(f\"\\n--- B. Concatenation (Spatial Map) ---\")\n",
        "    print(f\"   > Concat([h, w_perm]): {list(x_cat.shape)} <- Merges X and Y coordinates into one spatial map\")\n",
        "\n",
        "    # 5. Shared Convolution (Reduction)\n",
        "    reduction = 8\n",
        "    conv1 = nn.Conv2d(c, c // reduction, kernel_size=1)\n",
        "    bn1 = nn.BatchNorm2d(c // reduction)\n",
        "    act = nn.Hardswish()\n",
        "\n",
        "    f_spatial = act(bn1(conv1(x_cat)))\n",
        "    print(f\"\\n--- C. Shared Processing ---\")\n",
        "    print(f\"   > Conv1x1 -> BN -> HardSwish: {list(f_spatial.shape)} <- Learns spatial relationships\")\n",
        "\n",
        "    # 6. Split & Generate Attention Maps\n",
        "    x_h_prime, x_w_prime = torch.split(f_spatial, [h, w], dim=2)\n",
        "    x_w_prime = x_w_prime.permute(0, 1, 3, 2) # Flip back\n",
        "\n",
        "    conv_h = nn.Conv2d(c // reduction, c, kernel_size=1)\n",
        "    conv_w = nn.Conv2d(c // reduction, c, kernel_size=1)\n",
        "\n",
        "    a_h = torch.sigmoid(conv_h(x_h_prime))\n",
        "    a_w = torch.sigmoid(conv_w(x_w_prime))\n",
        "\n",
        "    print(f\"\\n--- D. Attention Generation ---\")\n",
        "    print(f\"   > Attn_H (Height Weights): {list(a_h.shape)} <- 'Which Rows are important?'\")\n",
        "    print(f\"   > Attn_W (Width Weights):  {list(a_w.shape)} <- 'Which Columns are important?'\")\n",
        "\n",
        "    # 7. Final Refinement\n",
        "    out = x * a_h * a_w\n",
        "    print(f\"\\n[OUTPUT] Refined Map: {list(out.shape)}\")\n",
        "\n",
        "\n",
        "# ==================================================================================\n",
        "# 2. DEEP INSPECTION: SimAM (The \"Energy\" Calculation)\n",
        "# ==================================================================================\n",
        "def inspect_simam():\n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"   2. SimAM: ENERGY CALCULATION (Math Logic)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Input\n",
        "    x = torch.randn(1, 32, 14, 14)\n",
        "    print(f\"[INPUT] Feature Map: {list(x.shape)}\")\n",
        "\n",
        "    n = x.shape[2] * x.shape[3] - 1\n",
        "    d = (x - x.mean(dim=[2,3], keepdim=True)).pow(2)\n",
        "\n",
        "    # FORMULA VARIABLES\n",
        "    # mu: Mean of the channel\n",
        "    # sigma^2: Variance of the channel\n",
        "    # t: The target neuron value\n",
        "    # lambda: Regularization constant\n",
        "\n",
        "    print(f\"\\n--- A. Statistical Analysis ---\")\n",
        "    print(f\"   > Calculating Mean (mu) per channel...\")\n",
        "    print(f\"   > Calculating Variance (sigma) per channel...\")\n",
        "\n",
        "    # Energy Formula: e_t = (t - mu)^2 / (4 * (sigma^2 + lambda)) + 0.5\n",
        "    e_lambda = 1e-4\n",
        "    v = d / (4 * (d.sum(dim=[2,3], keepdim=True) / n + e_lambda)) + 0.5\n",
        "\n",
        "    print(f\"\\n--- B. Energy Matrix (v) ---\")\n",
        "    print(f\"   > Shape: {list(v.shape)}\")\n",
        "    print(f\"   > Logic: Lower Energy = More Informative Neuron (Outlier from background)\")\n",
        "\n",
        "    # Attention = Sigmoid(1 / Energy)\n",
        "    attention_map = torch.sigmoid(v)\n",
        "\n",
        "    print(f\"\\n--- C. Attention Scaling ---\")\n",
        "    print(f\"   > Sigmoid(Energy): {list(attention_map.shape)}\")\n",
        "    print(f\"   > Result: Neurons with LOW energy get HIGH weights.\")\n",
        "\n",
        "    out = x * attention_map\n",
        "    print(f\"\\n[OUTPUT] Reweighted Features: {list(out.shape)}\")\n",
        "\n",
        "\n",
        "# ==================================================================================\n",
        "# 3. DEEP INSPECTION: KAN LINEAR (The \"Learnable Activation\")\n",
        "# ==================================================================================\n",
        "def inspect_kan_linear():\n",
        "    print(\"\\n\\n\" + \"=\"*80)\n",
        "    print(\"   3. KAN LINEAR: SPLINE MECHANICS (Detailed)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Setup\n",
        "    in_f = 32\n",
        "    out_f = 4 # 4 Classes\n",
        "    grid_size = 5\n",
        "\n",
        "    x = torch.randn(1, in_f) # Flattened vector\n",
        "    print(f\"[INPUT] Flattened Vector: {list(x.shape)}\")\n",
        "\n",
        "    # 1. Base Linear Path (SiLU)\n",
        "    base_weight = torch.randn(out_f, in_f)\n",
        "    base_output = F.linear(x, base_weight)\n",
        "    base_act = F.silu(x) # SiLU activation on input\n",
        "\n",
        "    print(f\"\\n--- A. Base Path (Residual) ---\")\n",
        "    print(f\"   > Operation: W_base * SiLU(x)\")\n",
        "    print(f\"   > Shape: {list(base_output.shape)}\")\n",
        "\n",
        "    # 2. Spline Path (The KAN Magic)\n",
        "    # In a real KAN, we compute B-Spline basis functions here.\n",
        "    # We expand dimensions to apply a different function for every input-output pair.\n",
        "\n",
        "    print(f\"\\n--- B. Spline Path (Learnable Functions) ---\")\n",
        "    print(f\"   > Grid Size: {grid_size} (Control points for the curve)\")\n",
        "\n",
        "    # Simulate Spline computation shape\n",
        "    # We expand X to match the Spline Weight Matrix [Out, In, Grid]\n",
        "    x_expanded = x.unsqueeze(1).expand(1, out_f, in_f)\n",
        "\n",
        "    spline_weights = torch.randn(out_f, in_f, grid_size)\n",
        "\n",
        "    print(f\"   > Input Expanded: {list(x_expanded.shape)}\")\n",
        "    print(f\"   > Spline Weights: {list(spline_weights.shape)} <- 3D Matrix (Learnable Curves)\")\n",
        "\n",
        "    # Aggregation\n",
        "    # Summing the contribution of splines for each output class\n",
        "    spline_output = torch.sum(spline_weights.mean(dim=2) * x.unsqueeze(1), dim=2)\n",
        "\n",
        "    print(f\"   > Spline Output: {list(spline_output.shape)}\")\n",
        "\n",
        "    # 3. Final Sum\n",
        "    final = base_output + spline_output\n",
        "    print(f\"\\n--- C. Aggregation ---\")\n",
        "    print(f\"   > Final = Base_Linear + Spline_Function\")\n",
        "    print(f\"   > Logits: {list(final.shape)} (Ready for Softmax)\")\n",
        "\n",
        "# ==================================================================================\n",
        "# 4. RUN ALL INSPECTIONS\n",
        "# ==================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    inspect_coordinate_attention()\n",
        "    inspect_simam()\n",
        "    inspect_kan_linear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdhRc67IUjI9",
        "outputId": "b68522c1-ff2a-4449-9052-461a58f41e7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "   1. COORDINATE ATTENTION: INNER MECHANICS (Step-by-Step)\n",
            "================================================================================\n",
            "[INPUT] Feature Map: [1, 32, 14, 14]\n",
            "\n",
            "--- A. Direction-Aware Pooling ---\n",
            "   > pool_h(x): [1, 32, 14, 1]  <- Compresses Width, keeps Row information\n",
            "   > pool_w(x): [1, 32, 1, 14]  <- Compresses Height, keeps Column information\n",
            "\n",
            "--- B. Concatenation (Spatial Map) ---\n",
            "   > Concat([h, w_perm]): [1, 32, 28, 1] <- Merges X and Y coordinates into one spatial map\n",
            "\n",
            "--- C. Shared Processing ---\n",
            "   > Conv1x1 -> BN -> HardSwish: [1, 4, 28, 1] <- Learns spatial relationships\n",
            "\n",
            "--- D. Attention Generation ---\n",
            "   > Attn_H (Height Weights): [1, 32, 14, 1] <- 'Which Rows are important?'\n",
            "   > Attn_W (Width Weights):  [1, 32, 1, 14] <- 'Which Columns are important?'\n",
            "\n",
            "[OUTPUT] Refined Map: [1, 32, 14, 14]\n",
            "\n",
            "\n",
            "================================================================================\n",
            "   2. SimAM: ENERGY CALCULATION (Math Logic)\n",
            "================================================================================\n",
            "[INPUT] Feature Map: [1, 32, 14, 14]\n",
            "\n",
            "--- A. Statistical Analysis ---\n",
            "   > Calculating Mean (mu) per channel...\n",
            "   > Calculating Variance (sigma) per channel...\n",
            "\n",
            "--- B. Energy Matrix (v) ---\n",
            "   > Shape: [1, 32, 14, 14]\n",
            "   > Logic: Lower Energy = More Informative Neuron (Outlier from background)\n",
            "\n",
            "--- C. Attention Scaling ---\n",
            "   > Sigmoid(Energy): [1, 32, 14, 14]\n",
            "   > Result: Neurons with LOW energy get HIGH weights.\n",
            "\n",
            "[OUTPUT] Reweighted Features: [1, 32, 14, 14]\n",
            "\n",
            "\n",
            "================================================================================\n",
            "   3. KAN LINEAR: SPLINE MECHANICS (Detailed)\n",
            "================================================================================\n",
            "[INPUT] Flattened Vector: [1, 32]\n",
            "\n",
            "--- A. Base Path (Residual) ---\n",
            "   > Operation: W_base * SiLU(x)\n",
            "   > Shape: [1, 4]\n",
            "\n",
            "--- B. Spline Path (Learnable Functions) ---\n",
            "   > Grid Size: 5 (Control points for the curve)\n",
            "   > Input Expanded: [1, 4, 32]\n",
            "   > Spline Weights: [4, 32, 5] <- 3D Matrix (Learnable Curves)\n",
            "   > Spline Output: [1, 4]\n",
            "\n",
            "--- C. Aggregation ---\n",
            "   > Final = Base_Linear + Spline_Function\n",
            "   > Logits: [1, 4] (Ready for Softmax)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "\n",
        "# ==================================================================================\n",
        "# 1. DEFINE MODEL COMPONENTS\n",
        "# ==================================================================================\n",
        "\n",
        "# --- Helpers ---\n",
        "class HardSwish(nn.Module):\n",
        "    def forward(self, x): return x * F.relu6(x + 3.) / 6.\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, in_c, r_dim):\n",
        "        super().__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_c, r_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(r_dim, in_c, 1), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x): return x * self.se(x)\n",
        "\n",
        "# --- Backbone Block ---\n",
        "class HybridMBConv(nn.Module):\n",
        "    def __init__(self, in_c, out_c, k, s, exp):\n",
        "        super().__init__()\n",
        "        self.stats = {'k': k, 's': s, 'exp': exp} # For printing\n",
        "        self.use_res = (s == 1 and in_c == out_c)\n",
        "        hid = int(in_c * exp)\n",
        "        layers = []\n",
        "        if exp != 1: layers.extend([nn.Conv2d(in_c, hid, 1, bias=False), nn.BatchNorm2d(hid), HardSwish()])\n",
        "        pad = (k - 1) // 2\n",
        "        layers.extend([nn.Conv2d(hid, hid, k, s, pad, groups=hid, bias=False), nn.BatchNorm2d(hid), HardSwish()])\n",
        "        layers.append(SqueezeExcitation(hid, hid // 4))\n",
        "        layers.extend([nn.Conv2d(hid, out_c, 1, bias=False), nn.BatchNorm2d(out_c)])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "    def forward(self, x): return x + self.conv(x) if self.use_res else self.conv(x)\n",
        "\n",
        "# --- Module 1: Backbone ---\n",
        "class EffiMobileBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(nn.Conv2d(3, 24, 3, 2, 1, bias=False), nn.BatchNorm2d(24), HardSwish())\n",
        "        # [in, out, k, s, exp]\n",
        "        config = [\n",
        "            [24, 24, 3, 1, 1], [24, 40, 3, 2, 4], [40, 40, 3, 1, 4],\n",
        "            [40, 80, 5, 2, 6], [80, 80, 5, 1, 6], [80, 80, 5, 1, 6],\n",
        "            [80, 112, 3, 1, 6], [112, 160, 5, 2, 6], [160, 160, 5, 1, 6], [160, 320, 3, 1, 6]\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*[HybridMBConv(*c) for c in config])\n",
        "        self.final_conv = nn.Sequential(nn.Conv2d(320, 1280, 1, bias=False), nn.BatchNorm2d(1280), HardSwish())\n",
        "    def forward(self, x): return self.final_conv(self.layers(self.stem(x)))\n",
        "\n",
        "# --- Module 2: Custom Channel Attention (Table 7) ---\n",
        "class CustomChannelAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels=128):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels, 16, 5, padding=2), nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(16, 16, 5, padding=2), nn.MaxPool2d(2, 2))\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(16, 32, 3, padding=1), nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(nn.Conv2d(32, 32, 3, padding=1), nn.Sigmoid())\n",
        "    def forward(self, x): return self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
        "\n",
        "# --- Module 3: Spatial Attention (Table 8) ---\n",
        "class SpatialAttentionModule(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        scale = self.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))\n",
        "        return x * scale\n",
        "\n",
        "# --- Module 4: KAN Head ---\n",
        "class KANLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=5):\n",
        "        super().__init__()\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size))\n",
        "        nn.init.kaiming_uniform_(self.base_weight, a=5**0.5); nn.init.normal_(self.spline_weight, 0.0, 1.0)\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.base_weight) + torch.sum(self.spline_weight.mean(dim=2) * x.unsqueeze(1), dim=2)\n",
        "\n",
        "# --- MAIN MODEL ---\n",
        "class LMD_KANet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.backbone = EffiMobileBackbone()\n",
        "        self.adapter = nn.Sequential(nn.Conv2d(1280, 128, 1, bias=False), nn.BatchNorm2d(128), nn.SiLU())\n",
        "        self.channel_att = CustomChannelAttentionBlock(128)\n",
        "        self.spatial_att = SpatialAttentionModule(7)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.kan = KANLinear(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.adapter(x)\n",
        "        x = self.channel_att(x)\n",
        "        x = self.spatial_att(x)\n",
        "        return self.kan(self.drop(self.pool(x).flatten(1)))\n",
        "\n",
        "# ==================================================================================\n",
        "# 2. MODULE-WISE INSPECTOR (The Code you asked for)\n",
        "# ==================================================================================\n",
        "def inspect_modules_separately(model):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\" LMD-KANet: MODULAR BREAKDOWN & SUMMARY\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    # Create Dummy Input\n",
        "    x = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "    # --- 1. BACKBONE INSPECTION ---\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"MODULE 1: HYBRID BACKBONE (EffiMobile)\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"{'Sub-Layer':<20} | {'Kernel':<10} | {'Input':<18} | {'Output':<18} | {'Params':<10}\")\n",
        "\n",
        "    # Stem\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.backbone.stem(x)\n",
        "    p = sum(p.numel() for p in model.backbone.stem.parameters())\n",
        "    print(f\"{'Stem':<20} | {'3x3':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # Blocks\n",
        "    for i, block in enumerate(model.backbone.layers):\n",
        "        in_shape = tuple(x.shape)\n",
        "        x = block(x)\n",
        "        p = sum(p.numel() for p in block.parameters())\n",
        "        k = f\"{block.stats['k']}x{block.stats['k']}\"\n",
        "        print(f\"{f'HybridMBConv {i+1}':<20} | {k:<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # Final\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.backbone.final_conv(x)\n",
        "    p = sum(p.numel() for p in model.backbone.final_conv.parameters())\n",
        "    print(f\"{'Final Expansion':<20} | {'1x1':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # --- 2. ADAPTER INSPECTION ---\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"MODULE 2: ADAPTER LAYER (Bridge)\")\n",
        "    print(\"-\"*80)\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.adapter(x)\n",
        "    p = sum(p.numel() for p in model.adapter.parameters())\n",
        "    print(f\"{'Conv1x1+BN':<20} | {'1x1':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # --- 3. CUSTOM CHANNEL ATTENTION INSPECTION ---\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"MODULE 3: CUSTOM CHANNEL ATTENTION (Table 7)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Layer 1\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.channel_att.layer1(x)\n",
        "    p = sum(p.numel() for p in model.channel_att.layer1.parameters())\n",
        "    print(f\"{'Layer 1 (Conv+Relu)':<20} | {'5x5':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # Layer 2\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.channel_att.layer2(x)\n",
        "    p = sum(p.numel() for p in model.channel_att.layer2.parameters())\n",
        "    print(f\"{'Layer 2 (Conv+Pool)':<20} | {'5x5':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # Layer 3\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.channel_att.layer3(x)\n",
        "    p = sum(p.numel() for p in model.channel_att.layer3.parameters())\n",
        "    print(f\"{'Layer 3 (Conv+Relu)':<20} | {'3x3':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # Layer 4\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.channel_att.layer4(x)\n",
        "    p = sum(p.numel() for p in model.channel_att.layer4.parameters())\n",
        "    print(f\"{'Layer 4 (Conv+Sigm)':<20} | {'3x3':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # --- 4. SPATIAL ATTENTION INSPECTION ---\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"MODULE 4: SPATIAL ATTENTION (Table 8)\")\n",
        "    print(\"-\"*80)\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.spatial_att(x)\n",
        "    p = sum(p.numel() for p in model.spatial_att.parameters())\n",
        "    print(f\"{'Spatial Map (7x7)':<20} | {'7x7':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    # --- 5. CLASSIFICATION HEAD INSPECTION ---\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"MODULE 5: KAN CLASSIFIER\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Pool/Drop\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.drop(model.pool(x).flatten(1))\n",
        "    print(f\"{'GlobalPool+Drop':<20} | {'-':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | 0\")\n",
        "\n",
        "    # KAN\n",
        "    in_shape = tuple(x.shape)\n",
        "    x = model.kan(x)\n",
        "    p = sum(p.numel() for p in model.kan.parameters())\n",
        "    print(f\"{'KAN Linear':<20} | {'Spline':<10} | {str(in_shape):<18} | {str(tuple(x.shape)):<18} | {p:,}\")\n",
        "\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Run it\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = LMD_KANet(num_classes=4).to(device)\n",
        "    inspect_modules_separately(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYmpxI1pTcr5",
        "outputId": "6dfded80-bcb8-4275-8965-cf515cc75bd5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            " LMD-KANet: MODULAR BREAKDOWN & SUMMARY\n",
            "========================================\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODULE 1: HYBRID BACKBONE (EffiMobile)\n",
            "--------------------------------------------------------------------------------\n",
            "Sub-Layer            | Kernel     | Input              | Output             | Params    \n",
            "Stem                 | 3x3        | (1, 3, 224, 224)   | (1, 24, 112, 112)  | 696\n",
            "HybridMBConv 1       | 3x3        | (1, 24, 112, 112)  | (1, 24, 112, 112)  | 1,206\n",
            "HybridMBConv 2       | 3x3        | (1, 24, 112, 112)  | (1, 40, 56, 56)    | 12,200\n",
            "HybridMBConv 3       | 3x3        | (1, 40, 56, 56)    | (1, 40, 56, 56)    | 27,960\n",
            "HybridMBConv 4       | 5x5        | (1, 40, 56, 56)    | (1, 80, 28, 28)    | 65,020\n",
            "HybridMBConv 5       | 5x5        | (1, 80, 28, 28)    | (1, 80, 28, 28)    | 206,680\n",
            "HybridMBConv 6       | 5x5        | (1, 80, 28, 28)    | (1, 80, 28, 28)    | 206,680\n",
            "HybridMBConv 7       | 3x3        | (1, 80, 28, 28)    | (1, 112, 28, 28)   | 214,424\n",
            "HybridMBConv 8       | 5x5        | (1, 112, 28, 28)   | (1, 160, 14, 14)   | 429,224\n",
            "HybridMBConv 9       | 5x5        | (1, 160, 14, 14)   | (1, 160, 14, 14)   | 797,360\n",
            "HybridMBConv 10      | 3x3        | (1, 160, 14, 14)   | (1, 320, 14, 14)   | 935,920\n",
            "Final Expansion      | 1x1        | (1, 320, 14, 14)   | (1, 1280, 14, 14)  | 412,160\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODULE 2: ADAPTER LAYER (Bridge)\n",
            "--------------------------------------------------------------------------------\n",
            "Conv1x1+BN           | 1x1        | (1, 1280, 14, 14)  | (1, 128, 14, 14)   | 164,096\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODULE 3: CUSTOM CHANNEL ATTENTION (Table 7)\n",
            "--------------------------------------------------------------------------------\n",
            "Layer 1 (Conv+Relu)  | 5x5        | (1, 128, 14, 14)   | (1, 16, 14, 14)    | 51,216\n",
            "Layer 2 (Conv+Pool)  | 5x5        | (1, 16, 14, 14)    | (1, 16, 7, 7)      | 6,416\n",
            "Layer 3 (Conv+Relu)  | 3x3        | (1, 16, 7, 7)      | (1, 32, 7, 7)      | 4,640\n",
            "Layer 4 (Conv+Sigm)  | 3x3        | (1, 32, 7, 7)      | (1, 32, 7, 7)      | 9,248\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODULE 4: SPATIAL ATTENTION (Table 8)\n",
            "--------------------------------------------------------------------------------\n",
            "Spatial Map (7x7)    | 7x7        | (1, 32, 7, 7)      | (1, 32, 7, 7)      | 98\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODULE 5: KAN CLASSIFIER\n",
            "--------------------------------------------------------------------------------\n",
            "GlobalPool+Drop      | -          | (1, 32, 7, 7)      | (1, 32)            | 0\n",
            "KAN Linear           | Spline     | (1, 32)            | (1, 4)             | 768\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}